{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/Spring2024/blob/main/Corpus/Words_in_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÉ Words in context\n",
        "\n",
        "## Key methods\n",
        "Analyzing words in context is fundamental for accurately interpreting and understanding language, whether in human communication, language learning, or computational language processing.\n",
        "\n",
        "+ Tokenization(Îã®Ïñ¥Ìôî)\n",
        "+ POS (Parts of Speech) Tagging\n",
        "+ Contextual Word Meaning\n",
        "+ Bigram, N-gram, Collocation\n",
        "+ Concordance(Îß•ÎùΩ ÏïàÏóêÏÑú ÏïûÎí§Î°ú Ïñ¥ÎñªÍ≤å Ïì∞Ïù¥ÎäîÏßÄ)\n"
      ],
      "metadata": {
        "id": "BC4851Myyn8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## {nltk} installation"
      ],
      "metadata": {
        "id": "B9mFGmLr0h9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "qGi1OEvQ0hox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtMBTSz_yi17"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize #sentence tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Tokenization\n",
        "\n",
        "+ Purpose: Breaking down text into individual words (tokens) is the first step in many NLP tasks.\n",
        "+ Method: Use nltk.word_tokenize() for tokenizing sentences into words."
      ],
      "metadata": {
        "id": "WJjs8pWk1EMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over 2 lazy dogs.\""
      ],
      "metadata": {
        "id": "kzln7GQ50uf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare **text.split()** and **word_tokenize**"
      ],
      "metadata": {
        "id": "vG7R_uhlGKm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mywords = text.split()\n",
        "print(mywords)\n",
        "print(len(mywords))"
      ],
      "metadata": {
        "id": "GUh6-XzYGDJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "QSCmaGF30-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out tokens that are not alphabetic"
      ],
      "metadata": {
        "id": "BF3ZnfbZHMzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out tokens that are not alphabetic\n",
        "words = [token for token in tokens if token.isalpha()]\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "id": "DKyBk677G8jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Part-of-Speech (POS) Tagging\n",
        "\n",
        "+ Purpose: Assigning parts of speech to each word (like noun, verb, adjective) helps in understanding the grammatical context.\n",
        "+ Method: Use nltk.pos_tag().\n",
        "+ [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ],
      "metadata": {
        "id": "mUv_Z0T91ANX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "2fY4uJTy1dtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "5IXJE9j_1SRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] Contextual Word Meaning (Word Sense Disambiguation):\n",
        "\n",
        "+ Purpose: Determining the meaning of a word based on the context it appears in.\n",
        "+ Method: Use algorithms like Lesk Algorithm implemented in NLTK.\n",
        "\n",
        "Note: NLTK uses [WordNet](https://wordnet.princeton.edu)"
      ],
      "metadata": {
        "id": "quRKdtmQ2HA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "M6kztW5gKKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Bank (Meaning 1 - Financial Institution):\n",
        "\n",
        "  + Sentence 1: I need to visit the bank to withdraw some money.\n",
        "  + Sentence 2: The bank of the river was a peaceful place to relax.\n",
        "+ Bat (Meaning 1 - Nocturnal Flying Mammal):\n",
        "\n",
        "  + Sentence 1: I saw a bat flying in the night sky.\n",
        "  + Sentence 2: She used a baseball bat to hit the ball out of the park.\n",
        "+ Book (Meaning 1 - Written or Printed Work):\n",
        "\n",
        "  + Sentence 1: I'm reading a fascinating book about space exploration.\n",
        "  + Sentence 2: Please book a table for two at the restaurant for tonight.\n",
        "+ Crane (Meaning 1 - Bird with a Long Neck):\n",
        "\n",
        "  + Sentence 1: A beautiful crane waded in the shallow water.\n",
        "  + Sentence 2: They used a crane to lift the heavy machinery onto the truck.\n",
        "+ Club (Meaning 1 - Social Organization):\n",
        "\n",
        "  + Sentence 1: I'm a member of the local chess club.\n",
        "  + Sentence 2: He used a golf club to hit the ball into the hole."
      ],
      "metadata": {
        "id": "E_rYv9ZJ3yLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example sentence: The bank of the river was a peaceful place to relax."
      ],
      "metadata": {
        "id": "wpgimrUdRHRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Paste a sentence: \")\n",
        "ambiguous = input(\"Type target word: \")\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "found = False  # Flag to indicate if 'bank' is found\n",
        "for word, tag in pos_tags:\n",
        "    if word.lower() == ambiguous:  # Using lower to make the search case-insensitive\n",
        "        print(\"POS tag of the ambiguous':\", tag)\n",
        "        found = True\n",
        "        break  # Exit the loop once 'bank' is found"
      ],
      "metadata": {
        "id": "0dowCbih4Kk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ example text (address): In the crowded conference room, as the quarterly meeting commenced, all eyes were on the project manager, Ms. Rivera. The recent challenges faced by the team had sparked concerns among the stakeholders, and it was her responsibility to address these issues comprehensively. She began by acknowledging the setbacks, carefully outlining the underlying causes, and elaborating on the steps taken to mitigate the impacts.\n",
        "+ example (match; game): The tennis match was intense, with both players demonstrating exceptional skill and endurance. Each set brought the crowd to its feet, cheering for incredible volleys and powerful serves. The excitement built with every point scored, highlighting the competitive spirit of the game.\n",
        "+ example (match; ignition): As the sun dipped below the horizon, Emma gathered twigs and dry leaves to start a campfire. She struck a match against the side of the box, watching as it flared brightly. Carefully, she touched the flame to the kindling, coaxing the small fire to life, its warm glow soon illuminating the campsite."
      ],
      "metadata": {
        "id": "bexnMQp0MEKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input()\n",
        "ambiguous = input()\n",
        "\n",
        "word_sense = lesk(word_tokenize(sentence), ambiguous)\n",
        "\n",
        "# Access the name of the disambiguated sense\n",
        "print(\"Disambiguated Sense:\", word_sense.name())\n",
        "# Access the definition of the disambiguated sense\n",
        "print(\"Sense Definition:\", word_sense.definition())\n"
      ],
      "metadata": {
        "id": "LyXk0_gx1_XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "sentence = \"You addressed the issue clearly.\"\n",
        "ambiguous_word = \"addressed\"\n",
        "\n",
        "# Define a function to map Penn Treebank POS tags to WordNet POS tags\n",
        "def penn_to_wordnet_pos(penn_pos):\n",
        "    if penn_pos.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif penn_pos.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif penn_pos.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif penn_pos.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    else:\n",
        "        return None  # Return None for unknown POS tags\n",
        "\n",
        "# Define your sentence and ambiguous word\n",
        "sentence = \"The invalid is in the hospital.\"\n",
        "ambiguous_word = \"invalid\"\n",
        "\n",
        "# Tokenize the sentence and perform POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n",
        "# Determine the Penn Treebank POS tag for the ambiguous word\n",
        "ambiguous_word_pos_penn = None\n",
        "\n",
        "for token, pos in pos_tags:\n",
        "    if token == ambiguous_word:\n",
        "        ambiguous_word_pos_penn = pos\n",
        "        break\n",
        "\n",
        "# Map the Penn Treebank POS tag to WordNet POS tag\n",
        "ambiguous_word_pos_wordnet = penn_to_wordnet_pos(ambiguous_word_pos_penn)\n",
        "\n",
        "if ambiguous_word_pos_wordnet is None:\n",
        "    print(f\"Cannot determine WordNet POS category for '{ambiguous_word_pos_penn}'.\")\n",
        "else:\n",
        "    # Retrieve synsets and disambiguate sense\n",
        "    synsets = wordnet.synsets(ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "\n",
        "    if synsets:\n",
        "        word_sense = lesk(tokens, ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "        print(\"Disambiguated Sense:\", word_sense.name())\n",
        "        print(\"Sense Definition:\", word_sense.definition())\n",
        "    else:\n",
        "        print(f\"No synsets found for '{ambiguous_word}' in the '{ambiguous_word_pos_wordnet}' category.\")\n"
      ],
      "metadata": {
        "id": "NweKOui59FfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio"
      ],
      "metadata": {
        "id": "pIFHPo14-w39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "jpQshXAV-2UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ example text (address): In the crowded conference room, as the quarterly meeting commenced, all eyes were on the project manager, Ms. Rivera. The recent challenges faced by the team had sparked concerns among the stakeholders, and it was her responsibility to address these issues comprehensively. She began by acknowledging the setbacks, carefully outlining the underlying causes, and elaborating on the steps taken to mitigate the impacts.\n",
        "+ example (match; game): The tennis match was intense, with both players demonstrating exceptional skill and endurance. Each set brought the crowd to its feet, cheering for incredible volleys and powerful serves. The excitement built with every point scored, highlighting the competitive spirit of the game.\n",
        "+ example (match; ignition): As the sun dipped below the horizon, Emma gathered twigs and dry leaves to start a campfire. She struck a match against the side of the box, watching as it flared brightly. Carefully, she touched the flame to the kindling, coaxing the small fire to life, its warm glow soon illuminating the campsite."
      ],
      "metadata": {
        "id": "VKO90kQ-i0Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Gradio app to display the ambiguous meaning (Not so reliable)\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Define a function to map Penn Treebank POS tags to WordNet POS tags\n",
        "def penn_to_wordnet_pos(penn_pos):\n",
        "    if penn_pos.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif penn_pos.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif penn_pos.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif penn_pos.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    else:\n",
        "        return None  # Return None for unknown POS tags\n",
        "\n",
        "# Define the disambiguation function that uses POS tagging\n",
        "def disambiguate_word_sense(sentence, ambiguous_word):\n",
        "    # Tokenize the sentence and perform POS tagging\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Find the POS tag for the ambiguous word in the tokenized sentence\n",
        "    ambiguous_word_pos_penn = None\n",
        "    for word, pos in pos_tags:\n",
        "        if word.lower() == ambiguous_word.lower():\n",
        "            ambiguous_word_pos_penn = pos\n",
        "            break\n",
        "\n",
        "    # If the POS tag is found, convert to WordNet POS tag\n",
        "    if ambiguous_word_pos_penn:\n",
        "        ambiguous_word_pos_wordnet = penn_to_wordnet_pos(ambiguous_word_pos_penn)\n",
        "    else:\n",
        "        return \"The ambiguous word was not found in the sentence.\"\n",
        "\n",
        "    if ambiguous_word_pos_wordnet:\n",
        "        # Perform Word Sense Disambiguation using Lesk algorithm\n",
        "        word_sense = lesk(tokens, ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "        if word_sense:\n",
        "            return f\"Disambiguated Sense: {word_sense.name()}\\nSense Definition: {word_sense.definition()}\"\n",
        "        else:\n",
        "            return f\"No disambiguated sense found for '{ambiguous_word}'.\"\n",
        "    else:\n",
        "        return f\"Cannot determine WordNet POS category for '{ambiguous_word}'.\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=disambiguate_word_sense,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter a sentence containing the ambiguous word\", label=\"Sentence\"),\n",
        "        gr.Textbox(placeholder=\"Enter the ambiguous word\", label=\"Ambiguous Word\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Result\"),\n",
        "    title=\"Word Sense Disambiguation\",\n",
        "    description=\"Enter a sentence and an ambiguous word to disambiguate its sense.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "o0LwagpzMBeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With POS (Just to get an idea)"
      ],
      "metadata": {
        "id": "8TtAnYv2Pq0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Gradio app\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Open Multilingual Wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Define the disambiguation function\n",
        "def disambiguate_word_sense(sentence, ambiguous_word):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Attempt to find a WordNet POS tag for the ambiguous word from the POS tags provided by NLTK\n",
        "    wordnet_pos = None\n",
        "    for word, tag in tagged_tokens:\n",
        "        if word.lower() == ambiguous_word.lower():\n",
        "            if tag.startswith('N'):\n",
        "                wordnet_pos = wordnet.NOUN\n",
        "            elif tag.startswith('V'):\n",
        "                wordnet_pos = wordnet.VERB\n",
        "            elif tag.startswith('J'):\n",
        "                wordnet_pos = wordnet.ADJ\n",
        "            elif tag.startswith('R'):\n",
        "                wordnet_pos = wordnet.ADV\n",
        "            break\n",
        "\n",
        "    # Use lesk to disambiguate the sense of the word\n",
        "    disambiguated_sense = lesk(tokens, ambiguous_word, pos=wordnet_pos)\n",
        "\n",
        "    if disambiguated_sense:\n",
        "        sense_name = disambiguated_sense.name()\n",
        "        sense_definition = disambiguated_sense.definition()  # Get the definition of the selected sense\n",
        "        return f\"Disambiguated Sense: {sense_name}\\nSense Definition: {sense_definition}\"\n",
        "    else:\n",
        "        return f\"No suitable sense found for '{ambiguous_word}'.\"\n",
        "\n",
        "# Create a Gradio interface with a submit button\n",
        "iface = gr.Interface(\n",
        "    fn=disambiguate_word_sense,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Sentence\", placeholder=\"Enter a sentence containing the ambiguous word\"),\n",
        "        gr.Textbox(label=\"Ambiguous Word\", placeholder=\"Enter the ambiguous word\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Result\"),\n",
        "    title=\"Word Sense Disambiguation\",\n",
        "    description=\"Enter a sentence and an ambiguous word to see its disambiguated sense based on context.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R-3pFy6dK0z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [4] Bi-gram, N-gram, and Collocation"
      ],
      "metadata": {
        "id": "1UR_9g6EVrHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ An \"ngram\" is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application.\n",
        "+ Ngrams are used in various applications like statistical language modeling, where they help predict the likelihood of a particular sequence of words.\n",
        "  + For example, in the sentence \"The quick brown fox jumps over the lazy dog,\" a 2-gram (or bigram) sequence would be (\"the quick\"), (\"quick brown\"), (\"brown fox\"), and so on.\n",
        "+ A \"collocation\" refers to a combination of words that occur together more frequently than would be expected by chance. This concept is often used in linguistic analysis to understand typical word combinations and patterns in language usage.\n"
      ],
      "metadata": {
        "id": "DLGLfP8vWxTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Bi-gram"
      ],
      "metadata": {
        "id": "NTn_44WjWjDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "ekyibE1QXEDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams_list = list(bigrams(tokens))\n",
        "\n",
        "# Print the bigrams\n",
        "for bg in bigrams_list:\n",
        "    print(bg)\n"
      ],
      "metadata": {
        "id": "Jzq1KWAHWl1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ N-gram"
      ],
      "metadata": {
        "id": "Xp2XaVytXVIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Make sure to download the required NLTK models and data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to get n-grams from text\n",
        "def get_ngrams(text, n):\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Generate n-grams\n",
        "    n_grams = ngrams(tokens, n)\n",
        "    # Convert to a list and return\n",
        "    return list(n_grams)\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Get bigrams (2-grams)\n",
        "bigrams = get_ngrams(text, 2)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Get trigrams (3-grams)\n",
        "trigrams = get_ngrams(text, 3)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "# Get 4-grams\n",
        "fourgrams = get_ngrams(text, 4)\n",
        "print(\"4-grams:\", fourgrams)\n"
      ],
      "metadata": {
        "id": "glNxZEoTXUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Collocation: Collocation in linguistics refers to the tendency of certain words to occur frequently together in a language. These word combinations often bear a meaning that is not entirely deducible from the individual words' meanings."
      ],
      "metadata": {
        "id": "L0pEAD8rXqDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk   # Install this if you haven't.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "mdQYDcjNZ1Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Python is an interpreted high-level general-purpose programming language. \\\n",
        "Python's design philosophy emphasizes code readability with its notable use of significant indentation. \\\n",
        "Its language constructs and object-oriented approach aim to help programmers write clear, \\\n",
        "logical code for small and large-scale projects. Python is dynamically-typed and garbage-collected. \\\n",
        "It supports multiple programming paradigms, including structured (particularly procedural), \\\n",
        "object-oriented, and functional programming. \\\n",
        "Python is often described as a \"batteries included\" language due to its comprehensive standard library. \\\n",
        "Python was created in the late 1980s as a successor to the ABC language. Python 2.0, released in 2000, \\\n",
        "introduced features like list comprehensions and a garbage collection system capable of collecting reference cycles. \\\n",
        "Python 3.0, released in 2008, was a major revision of the language that is not completely backward-compatible. \\\n",
        "The Python 2 series ended with version 2.7 in 2020. Python consistently ranks as one of the most popular programming languages.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "sDC18R9dajKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = word_tokenize(text.lower())  # Lowercasing for consistency\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "# Finding bigram collocations\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(filtered_tokens)\n",
        "finder.apply_freq_filter(2)  # Optional: filter out bigrams that occur less than 2 times\n",
        "\n",
        "# Display the 5 most frequent bigrams\n",
        "print(\"Top 5 bigram collocations:\")\n",
        "print(finder.nbest(bigram_measures.raw_freq, 5))\n",
        "\n",
        "# Finding trigram collocations\n",
        "trigram_measures = TrigramAssocMeasures()\n",
        "finder_tri = TrigramCollocationFinder.from_words(filtered_tokens)\n",
        "finder_tri.apply_freq_filter(2)  # Optional: filter out trigrams that occur less than 2 times\n",
        "\n",
        "# Display the 5 most frequent trigrams\n",
        "print(\"\\nTop 5 trigram collocations:\")\n",
        "print(finder_tri.nbest(trigram_measures.raw_freq, 5))\n"
      ],
      "metadata": {
        "id": "hVHvrne_aAXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Concordance(Words in Context)\n",
        "\n",
        "+ A \"concordance\" is a list of all occurrences of a particular search term in a corpus, presented together with a certain amount of context. This is often used in linguistic analysis to understand how words are used in different contexts."
      ],
      "metadata": {
        "id": "XO6sk1zfZiGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# # Ensure that the necessary NLTK data is available\n",
        "# nltk.download('punkt')\n",
        "\n",
        "from nltk.text import Text\n",
        "\n",
        "# Define a function to display concordances for a word in a given text\n",
        "def display_concordance(text, word, width=75, lines=25):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Create an NLTK text object\n",
        "    nltk_text = Text(tokens)\n",
        "    # Display concordances\n",
        "    nltk_text.concordance(word, width=width, lines=lines)\n",
        "\n",
        "\n",
        "# Display concordances for the word 'Python'\n",
        "display_concordance(text, 'Python')\n"
      ],
      "metadata": {
        "id": "y5P7DFWpXpvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ sample text: Jessica always wondered what life would be like in a different city. She thought she would enjoy the bustling streets and diverse culture, but there was always a part of her that feared the unknown. When her friend Mark suggested they visit New York for a week, she knew this would be her chance to experience city life firsthand. During their trip, she discovered she would indeed love the vibrant energy, although she also realized she would miss the quiet of her small hometown. Mark said he would probably move to the city in the future, but Jessica decided she would return home, now certain of where she belonged."
      ],
      "metadata": {
        "id": "IIuuLx_0l7I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mytext = input()"
      ],
      "metadata": {
        "id": "ZX1Ob-ZEl5u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_concordance(mytext, 'would', width=100, lines=25)"
      ],
      "metadata": {
        "id": "N4x-6xqImeIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### The END"
      ],
      "metadata": {
        "id": "SxI37Tsem4SG"
      }
    }
  ]
}